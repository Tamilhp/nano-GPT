# High-Performance GPT Built from Scratch

![Decoder-Only Transformer Architecture](https://dugas.ch/artificial_curiosity/img/GPT_architecture/GPT1.png)
## Overview

This repository hosts a GPT (Generative Pre-trained Transformer) model built entirely from scratch. The project aims to demonstrate the capabilities of a decoder-only transformer architecture in text generation tasks. It serves as a proof of concept and has been benchmarked against competitive performance metrics.

## ðŸŒŸ Key Features

- **Decoder-Only Transformer Architecture**: The core of the project is a GPT model built from scratch, utilizing a decoder-only transformer architecture. This design choice enables the model to excel in text generation tasks.
  
- **High-Performance Metrics**: Rigorous testing and benchmarking have been conducted to ensure the model's reliability and efficiency. The model has met and exceeded industry standards.

- **Text Generation Capabilities**: The model is capable of various text-based applications, from chatbots and virtual assistants to content generation and summarization.

## ðŸ›  Installation

To get started with this project, clone the repository and install the required packages:

```bash
git clone https://github.com/Tamilhp/nano-GPT.git
cd nano-GPT
```



## ðŸ“œ License

This project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for details.
